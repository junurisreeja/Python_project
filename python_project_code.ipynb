{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB36qjNfPrnd0p1gyxq2vz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junurisreeja/Python_project/blob/main/python_project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_wR4pgWEPzw",
        "outputId": "721a5f30-9dcb-45c7-d96a-eabf2cb647c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data shape: (953, 24)\n",
            "After cleaning shape: (953, 24)\n",
            "Target distributions:\n",
            " chart_any: {1: 0.9412381951731374, 0: 0.05876180482686254}\n",
            " playlist_top_quartile: {0: 0.7492130115424974, 1: 0.25078698845750264}\n",
            "Spearman(artist_count, streams): r=-0.1565, p=1.2e-06\n",
            "Features (numeric): ['artist_count', 'released_year', 'released_month', 'released_day', 'bpm', 'danceability_%', 'valence_%', 'energy_%', 'acousticness_%', 'instrumentalness_%', 'liveness_%', 'speechiness_%']\n",
            "Categorical: ['key', 'mode']\n",
            "\n",
            "=== Running TaskA_chart_any ===\n",
            "=== TaskA_chart_any | LogisticRegression ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0619    0.4286    0.1081        14\n",
            "           1     0.9437    0.5956    0.7302       225\n",
            "\n",
            "    accuracy                         0.5858       239\n",
            "   macro avg     0.5028    0.5121    0.4192       239\n",
            "weighted avg     0.8920    0.5858    0.6938       239\n",
            "\n",
            "=== TaskA_chart_any | LinearSVC ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0638    0.4286    0.1111        14\n",
            "           1     0.9448    0.6089    0.7405       225\n",
            "\n",
            "    accuracy                         0.5983       239\n",
            "   macro avg     0.5043    0.5187    0.4258       239\n",
            "weighted avg     0.8932    0.5983    0.7037       239\n",
            "\n",
            "=== TaskA_chart_any | SVC_rbf ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0769    0.1429    0.1000        14\n",
            "           1     0.9437    0.8933    0.9178       225\n",
            "\n",
            "    accuracy                         0.8494       239\n",
            "   macro avg     0.5103    0.5181    0.5089       239\n",
            "weighted avg     0.8929    0.8494    0.8699       239\n",
            "\n",
            "=== TaskA_chart_any | KNN ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000        14\n",
            "           1     0.9414    1.0000    0.9698       225\n",
            "\n",
            "    accuracy                         0.9414       239\n",
            "   macro avg     0.4707    0.5000    0.4849       239\n",
            "weighted avg     0.8863    0.9414    0.9130       239\n",
            "\n",
            "=== TaskA_chart_any | DecisionTree ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0556    0.0714    0.0625        14\n",
            "           1     0.9412    0.9244    0.9327       225\n",
            "\n",
            "    accuracy                         0.8745       239\n",
            "   macro avg     0.4984    0.4979    0.4976       239\n",
            "weighted avg     0.8893    0.8745    0.8818       239\n",
            "\n",
            "=== TaskA_chart_any | RandomForest ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000        14\n",
            "           1     0.9414    1.0000    0.9698       225\n",
            "\n",
            "    accuracy                         0.9414       239\n",
            "   macro avg     0.4707    0.5000    0.4849       239\n",
            "weighted avg     0.8863    0.9414    0.9130       239\n",
            "\n",
            "=== TaskA_chart_any | GradientBoosting ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000        14\n",
            "           1     0.9404    0.9822    0.9609       225\n",
            "\n",
            "    accuracy                         0.9247       239\n",
            "   macro avg     0.4702    0.4911    0.4804       239\n",
            "weighted avg     0.8853    0.9247    0.9046       239\n",
            "\n",
            "=== TaskA_chart_any | GaussianNB ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000        14\n",
            "           1     0.9320    0.8533    0.8910       225\n",
            "\n",
            "    accuracy                         0.8033       239\n",
            "   macro avg     0.4660    0.4267    0.4455       239\n",
            "weighted avg     0.8774    0.8033    0.8388       239\n",
            "\n",
            "Saved RF importances: results/rf_feature_importance_TaskA_chart_any.csv\n",
            "Saved LR coefficients: results/lr_coefficients_TaskA_chart_any.csv\n",
            "\n",
            "=== Running TaskB_playlist_top_quartile ===\n",
            "=== TaskB_playlist_top_quartile | LogisticRegression ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8258    0.8212    0.8235       179\n",
            "           1     0.4754    0.4833    0.4793        60\n",
            "\n",
            "    accuracy                         0.7364       239\n",
            "   macro avg     0.6506    0.6523    0.6514       239\n",
            "weighted avg     0.7379    0.7364    0.7371       239\n",
            "\n",
            "=== TaskB_playlist_top_quartile | LinearSVC ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8045    0.8045    0.8045       179\n",
            "           1     0.4167    0.4167    0.4167        60\n",
            "\n",
            "    accuracy                         0.7071       239\n",
            "   macro avg     0.6106    0.6106    0.6106       239\n",
            "weighted avg     0.7071    0.7071    0.7071       239\n",
            "\n",
            "=== TaskB_playlist_top_quartile | SVC_rbf ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8227    0.9330    0.8743       179\n",
            "           1     0.6667    0.4000    0.5000        60\n",
            "\n",
            "    accuracy                         0.7992       239\n",
            "   macro avg     0.7447    0.6665    0.6872       239\n",
            "weighted avg     0.7835    0.7992    0.7804       239\n",
            "\n",
            "=== TaskB_playlist_top_quartile | KNN ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7854    0.9609    0.8643       179\n",
            "           1     0.6500    0.2167    0.3250        60\n",
            "\n",
            "    accuracy                         0.7741       239\n",
            "   macro avg     0.7177    0.5888    0.5947       239\n",
            "weighted avg     0.7514    0.7741    0.7289       239\n",
            "\n",
            "=== TaskB_playlist_top_quartile | DecisionTree ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8272    0.8827    0.8541       179\n",
            "           1     0.5625    0.4500    0.5000        60\n",
            "\n",
            "    accuracy                         0.7741       239\n",
            "   macro avg     0.6949    0.6663    0.6770       239\n",
            "weighted avg     0.7608    0.7741    0.7652       239\n",
            "\n",
            "=== TaskB_playlist_top_quartile | RandomForest ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8341    0.9553    0.8906       179\n",
            "           1     0.7647    0.4333    0.5532        60\n",
            "\n",
            "    accuracy                         0.8243       239\n",
            "   macro avg     0.7994    0.6943    0.7219       239\n",
            "weighted avg     0.8167    0.8243    0.8059       239\n",
            "\n",
            "=== TaskB_playlist_top_quartile | GradientBoosting ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8571    0.9385    0.8960       179\n",
            "           1     0.7442    0.5333    0.6214        60\n",
            "\n",
            "    accuracy                         0.8368       239\n",
            "   macro avg     0.8007    0.7359    0.7587       239\n",
            "weighted avg     0.8288    0.8368    0.8271       239\n",
            "\n",
            "=== TaskB_playlist_top_quartile | GaussianNB ===\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7705    0.7877    0.7790       179\n",
            "           1     0.3214    0.3000    0.3103        60\n",
            "\n",
            "    accuracy                         0.6653       239\n",
            "   macro avg     0.5460    0.5439    0.5447       239\n",
            "weighted avg     0.6578    0.6653    0.6614       239\n",
            "\n",
            "Saved RF importances: results/rf_feature_importance_TaskB_playlist_top_quartile.csv\n",
            "Saved LR coefficients: results/lr_coefficients_TaskB_playlist_top_quartile.csv\n",
            "Saved metrics summary to: results/classification_metrics_summary.csv\n",
            "Saved report: results/classification_reports_TaskA_chart_any.txt\n",
            "Saved report: results/classification_reports_TaskB_playlist_top_quartile.txt\n",
            "Saved plot: results/f1_by_model_task.png\n",
            "\n",
            "Done. All outputs are in the 'results' folder.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "train_all_classifiers.py\n",
        "\n",
        "Implements multiple supervised classifiers and outputs classification reports\n",
        "for the \"Artist Collaboration and Popularity Boost\" problem using the\n",
        "'spotify 2023.csv.bz2' dataset.\n",
        "\n",
        "Outputs:\n",
        " - results/classification_metrics_summary.csv\n",
        " - results/classification_reports_taskA.txt\n",
        " - results/classification_reports_taskB.txt\n",
        " - results/*.png (plots)\n",
        " - results/rf_feature_importance_*.csv\n",
        " - results/lr_coefficients_*.csv\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# classifiers\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# stats for EDA\n",
        "from scipy.stats import spearmanr, f_oneway, chi2_contingency\n",
        "\n",
        "# -----------------------\n",
        "# Config\n",
        "# -----------------------\n",
        "DATA_PATH = \"/content/spotify-2023.csv\"   # update if file is elsewhere\n",
        "OUT_DIR = Path(\"results\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.25\n",
        "\n",
        "# -----------------------\n",
        "# 1) Load dataset robustly\n",
        "# -----------------------\n",
        "def load_df(path):\n",
        "    try:\n",
        "        df = pd.read_csv(path, encoding=\"latin1\")\n",
        "    except Exception:\n",
        "        df = pd.read_csv(path, encoding=\"latin1\", engine=\"python\", on_bad_lines=\"skip\")\n",
        "    return df\n",
        "\n",
        "df = load_df(DATA_PATH)\n",
        "print(\"Loaded data shape:\", df.shape)\n",
        "\n",
        "# normalize column names\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "# -----------------------\n",
        "# 2) Clean & coerce types\n",
        "# -----------------------\n",
        "# numeric columns we expect; some may be missing depending on CSV\n",
        "numeric_cols = [\n",
        "    \"artist_count\",\"released_year\",\"released_month\",\"released_day\",\n",
        "    \"in_spotify_playlists\",\"in_spotify_charts\",\n",
        "    \"in_apple_playlists\",\"in_apple_charts\",\n",
        "    \"in_deezer_playlists\",\"in_deezer_charts\",\"in_shazam_charts\",\n",
        "    \"bpm\",\"danceability_%\",\"valence_%\",\"energy_%\",\n",
        "    \"acousticness_%\",\"instrumentalness_%\",\"liveness_%\",\"speechiness_%\",\"streams\"\n",
        "]\n",
        "\n",
        "# sanitize streams (remove commas/non-digits) and coerce numeric\n",
        "if \"streams\" in df.columns:\n",
        "    df[\"streams\"] = df[\"streams\"].astype(str).str.replace(\",\", \"\", regex=False).str.replace(r\"[^\\d.]\", \"\", regex=True).replace(\"\", np.nan)\n",
        "\n",
        "for c in numeric_cols:\n",
        "    if c in df.columns:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# basic cleaning for categorical cols\n",
        "if \"mode\" in df.columns:\n",
        "    df[\"mode\"] = df[\"mode\"].astype(str).str.strip()\n",
        "if \"key\" in df.columns:\n",
        "    df[\"key\"] = df[\"key\"].astype(str).str.strip()\n",
        "\n",
        "# drop rows without artist_count (can't study collaboration)\n",
        "if \"artist_count\" in df.columns:\n",
        "    df = df[~df[\"artist_count\"].isna()].copy()\n",
        "\n",
        "print(\"After cleaning shape:\", df.shape)\n",
        "\n",
        "# -----------------------\n",
        "# 3) Targets (two tasks)\n",
        "# -----------------------\n",
        "# Task A: chart presence on ANY platform\n",
        "df[\"chart_any\"] = 0\n",
        "for c in [\"in_spotify_charts\",\"in_apple_charts\",\"in_deezer_charts\",\"in_shazam_charts\"]:\n",
        "    if c in df.columns:\n",
        "        df[\"chart_any\"] = df[\"chart_any\"] | (df[c].fillna(0) > 0)\n",
        "df[\"chart_any\"] = df[\"chart_any\"].astype(int)\n",
        "\n",
        "# Task B: top quartile of total playlist inclusions (spotify+apple+deezer)\n",
        "df[\"total_playlists\"] = 0\n",
        "for c in [\"in_spotify_playlists\",\"in_apple_playlists\",\"in_deezer_playlists\"]:\n",
        "    if c in df.columns:\n",
        "        df[\"total_playlists\"] = df[\"total_playlists\"] + df[c].fillna(0)\n",
        "df[\"playlist_top_quartile\"] = (df[\"total_playlists\"] >= df[\"total_playlists\"].quantile(0.75)).astype(int)\n",
        "\n",
        "print(\"Target distributions:\")\n",
        "print(\" chart_any:\", df[\"chart_any\"].value_counts(normalize=True).to_dict())\n",
        "print(\" playlist_top_quartile:\", df[\"playlist_top_quartile\"].value_counts(normalize=True).to_dict())\n",
        "\n",
        "# -----------------------\n",
        "# 4) Exploratory checks (optional)\n",
        "# -----------------------\n",
        "# Spearman correlation (artist_count vs streams) if streams present\n",
        "if \"streams\" in df.columns:\n",
        "    df_tmp = df.dropna(subset=[\"artist_count\",\"streams\"])\n",
        "    if not df_tmp.empty:\n",
        "        r, p = spearmanr(df_tmp[\"artist_count\"], df_tmp[\"streams\"])\n",
        "        print(f\"Spearman(artist_count, streams): r={r:.4f}, p={p:.4g}\")\n",
        "\n",
        "# -----------------------\n",
        "# 5) Modeling pipeline\n",
        "# -----------------------\n",
        "# Feature candidates (artist_count is included - the independent variable of interest)\n",
        "feature_candidates = [\n",
        "    \"artist_count\", \"released_year\", \"released_month\", \"released_day\",\n",
        "    \"bpm\", \"danceability_%\", \"valence_%\", \"energy_%\",\n",
        "    \"acousticness_%\", \"instrumentalness_%\", \"liveness_%\", \"speechiness_%\"\n",
        "]\n",
        "features = [c for c in feature_candidates if c in df.columns]\n",
        "categorical = [c for c in [\"key\",\"mode\"] if c in df.columns]\n",
        "\n",
        "print(\"Features (numeric):\", features)\n",
        "print(\"Categorical:\", categorical)\n",
        "\n",
        "# create X (features) once; will be sliced per train/test\n",
        "X_all = df[features + categorical].copy()\n",
        "\n",
        "# preprocessing\n",
        "num_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "cat_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"num\", num_pipeline, features),\n",
        "    (\"cat\", cat_pipeline, categorical)\n",
        "])\n",
        "\n",
        "# models to run\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
        "    \"LinearSVC\": LinearSVC(class_weight=\"balanced\", max_iter=10000, random_state=RANDOM_STATE),\n",
        "    \"SVC_rbf\": SVC(kernel=\"rbf\", class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=7),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=300, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE),\n",
        "    \"GradientBoosting\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
        "    \"GaussianNB\": GaussianNB()\n",
        "}\n",
        "\n",
        "tasks = {\n",
        "    \"TaskA_chart_any\": \"chart_any\",\n",
        "    \"TaskB_playlist_top_quartile\": \"playlist_top_quartile\"\n",
        "}\n",
        "\n",
        "# storage for metrics and reports\n",
        "metrics_rows = []\n",
        "reports = {task: [] for task in tasks.keys()}\n",
        "\n",
        "# we will also save RF importances and LR coefficients per task\n",
        "for task_label, target_col in tasks.items():\n",
        "    print(f\"\\n=== Running {task_label} ===\")\n",
        "    y = df[target_col].copy()\n",
        "    if y.nunique() < 2:\n",
        "        print(f\"Skipping {task_label} (only one class present).\")\n",
        "        continue\n",
        "\n",
        "    # split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_all, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        # For GaussianNB we will ensure dense arrays via FunctionTransformer (inside pipeline)\n",
        "        if model_name == \"GaussianNB\":\n",
        "            pipe = Pipeline([(\"pre\", preprocessor),\n",
        "                             (\"to_dense\", FunctionTransformer(lambda x: x.toarray() if hasattr(x, \"toarray\") else x)),\n",
        "                             (\"clf\", model)])\n",
        "        else:\n",
        "            pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", model)])\n",
        "\n",
        "        # fit & predict\n",
        "        pipe.fit(X_train, y_train)\n",
        "        y_pred = pipe.predict(X_test)\n",
        "\n",
        "        # metrics\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "        metrics_rows.append({\n",
        "            \"task\": task_label,\n",
        "            \"model\": model_name,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision\": prec,\n",
        "            \"recall\": rec,\n",
        "            \"f1\": f1\n",
        "        })\n",
        "\n",
        "        # textual classification report\n",
        "        rep = classification_report(y_test, y_pred, digits=4, zero_division=0)\n",
        "        header = f\"=== {task_label} | {model_name} ===\\n\"\n",
        "        reports[task_label].append(header + rep)\n",
        "        print(header)\n",
        "        print(rep)\n",
        "\n",
        "    # Feature importance (RandomForest) and LR coefficients\n",
        "    if \"RandomForest\" in models:\n",
        "        rf_pipe = Pipeline([(\"pre\", preprocessor), (\"rf\", RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE))])\n",
        "        rf_pipe.fit(X_train, y_train)\n",
        "        # get feature names\n",
        "        num_names = features\n",
        "        ohe_names = []\n",
        "        if categorical:\n",
        "            ohe = rf_pipe.named_steps[\"pre\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "            ohe_names = list(ohe.get_feature_names_out(categorical))\n",
        "        final_feature_names = num_names + ohe_names\n",
        "        importances = rf_pipe.named_steps[\"rf\"].feature_importances_\n",
        "        fi_df = pd.DataFrame({\"feature\": final_feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
        "        fi_df.to_csv(OUT_DIR / f\"rf_feature_importance_{task_label}.csv\", index=False)\n",
        "        print(f\"Saved RF importances: {OUT_DIR / f'rf_feature_importance_{task_label}.csv'}\")\n",
        "\n",
        "    if \"LogisticRegression\" in models:\n",
        "        lr_pipe = Pipeline([(\"pre\", preprocessor), (\"lr\", LogisticRegression(max_iter=2000, random_state=RANDOM_STATE))])\n",
        "        lr_pipe.fit(X_train, y_train)\n",
        "        ohe_names = []\n",
        "        if categorical:\n",
        "            ohe = lr_pipe.named_steps[\"pre\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "            ohe_names = list(ohe.get_feature_names_out(categorical))\n",
        "        final_feature_names = features + ohe_names\n",
        "        coefs = lr_pipe.named_steps[\"lr\"].coef_[0]\n",
        "        coef_df = pd.DataFrame({\"feature\": final_feature_names, \"coefficient\": coefs}).sort_values(\"coefficient\", key=abs, ascending=False)\n",
        "        coef_df.to_csv(OUT_DIR / f\"lr_coefficients_{task_label}.csv\", index=False)\n",
        "        print(f\"Saved LR coefficients: {OUT_DIR / f'lr_coefficients_{task_label}.csv'}\")\n",
        "\n",
        "# -----------------------\n",
        "# 6) Save metrics & reports\n",
        "# -----------------------\n",
        "metrics_df = pd.DataFrame(metrics_rows).sort_values([\"task\", \"f1\"], ascending=[True, False])\n",
        "metrics_df.to_csv(OUT_DIR / \"classification_metrics_summary.csv\", index=False)\n",
        "print(\"Saved metrics summary to:\", OUT_DIR / \"classification_metrics_summary.csv\")\n",
        "\n",
        "# save textual classification reports\n",
        "for task_label, reps in reports.items():\n",
        "    if reps:\n",
        "        with open(OUT_DIR / f\"classification_reports_{task_label}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\\n\".join(reps))\n",
        "        print(\"Saved report:\", OUT_DIR / f\"classification_reports_{task_label}.txt\")\n",
        "\n",
        "# -----------------------\n",
        "# 7) Basic visualization\n",
        "# -----------------------\n",
        "if not metrics_df.empty:\n",
        "    plt.figure(figsize=(10,6))\n",
        "    sns.barplot(data=metrics_df, x=\"model\", y=\"f1\", hue=\"task\")\n",
        "    plt.title(\"F1 by model and task\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_DIR / \"f1_by_model_task.png\")\n",
        "    plt.close()\n",
        "    print(\"Saved plot:\", OUT_DIR / \"f1_by_model_task.png\")\n",
        "\n",
        "print(\"\\nDone. All outputs are in the 'results' folder.\")\n"
      ]
    }
  ]
}